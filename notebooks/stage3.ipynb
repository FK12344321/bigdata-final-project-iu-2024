{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0635285d-5589-4eeb-9091-9dd2143b3492",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connect to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d77bfb-132e-422c-9933-69d0df5685dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = \"team23\"\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0a0875-0d8c-4f48-a65b-230e11c46d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-01.uni.innopolis.ru:4143\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>team23 - spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe8cc098390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc89ee-c475-47d1-b000-a800563a8870",
   "metadata": {},
   "source": [
    "# list Hive databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fddee3f-db6b-4c48-95f8-c480879868f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', description='Default Hive database', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/apps/hive/warehouse'), Database(name='root_db', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/root/root_db'), Database(name='team0_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team0/project/hive/warehouse'), Database(name='team12_hive_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team12/project/hive/warehouse'), Database(name='team13_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team13/project/hive/warehouse'), Database(name='team14_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team14/project/hive/warehouse'), Database(name='team15_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team15/project/hive/warehouse'), Database(name='team16_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team16/project/hive/warehouse'), Database(name='team17_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team17/project/hive/warehouse'), Database(name='team18_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team18/project/hive/warehouse'), Database(name='team19_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team19/project/hive/warehouse'), Database(name='team1_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team1/project/hive/warehouse'), Database(name='team20_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team20/project/hive/warehouse'), Database(name='team21_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team21/project/hive/warehouse'), Database(name='team22_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team22/project/hive/warehouse'), Database(name='team23_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/hive/warehouse'), Database(name='team24_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team24/project/hive/warehouse'), Database(name='team25_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team25/project/hive/warehouse'), Database(name='team26_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team26/project/hive/warehouse'), Database(name='team27_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team27/project/hive/warehouse'), Database(name='team28_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team28/project/hive/warehouse'), Database(name='team29_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team29/project/hive/warehouse'), Database(name='team2_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team2/project/hive/warehouse'), Database(name='team30_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team30/project/hive/warehouse'), Database(name='team31_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team31/project/hive/warehouse'), Database(name='team3_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team3/project/hive/warehouse'), Database(name='team4_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team4/project/hive/warehouse'), Database(name='team5_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team5/project/hive/warehouse'), Database(name='team6_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team6/project/hive/warehouse'), Database(name='team7_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team7/project/hive/warehouse'), Database(name='team8_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team8/project/hive/warehouse'), Database(name='team9_projectdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/user/team9/project/hive/warehouse'), Database(name='testdb', description='', locationUri='hdfs://hadoop-02.uni.innopolis.ru:8020/apps/hive/warehouse/testdb.db')]\n",
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             root_db|\n",
      "|     team0_projectdb|\n",
      "|team12_hive_proje...|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "|    team21_projectdb|\n",
      "|    team22_projectdb|\n",
      "|    team23_projectdb|\n",
      "|    team24_projectdb|\n",
      "|    team25_projectdb|\n",
      "|    team26_projectdb|\n",
      "|    team27_projectdb|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listDatabases())\n",
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05550202-1e06-409b-8810-947220880cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----------+\n",
      "|       namespace|       tableName|isTemporary|\n",
      "+----------------+----------------+-----------+\n",
      "|team23_projectdb|airbnb_part_buck|      false|\n",
      "|team23_projectdb|      q1_results|      false|\n",
      "|team23_projectdb|      q2_results|      false|\n",
      "|team23_projectdb|      q3_results|      false|\n",
      "|team23_projectdb|      q4_results|      false|\n",
      "|team23_projectdb|      q5_results|      false|\n",
      "+----------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE team23_projectdb;\")\n",
    "spark.sql(\"SHOW TABLES;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a1fe4-d2d3-4fb0-9533-59f61a917d05",
   "metadata": {},
   "source": [
    "# Specify the input and output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018f48d1-6fde-4ff5-b7ce-4eed719a7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the following features\n",
    "# Excluded 'thumbnail_url' and 'id' because it has no valuable information to extract\n",
    "# Exclude? host-related attributes since they contain little information about listing itself\n",
    "# Excluded 'first_review', 'host_response_rate', 'last_review' 'review_scores_rating'  because of large amount of Null values\n",
    "features = ['property_type', 'room_type', 'amenities', 'accommodates', 'bathrooms', 'bed_type', 'cancellation_policy',\\\n",
    "            'cleaning_fee', 'city', 'description', 'host_has_profile_pic', 'host_identity_verified','host_since',\\\n",
    "            'instant_bookable', 'latitude', 'longitude', 'name', 'neighbourhood', 'number_of_reviews', 'zipcode', 'beds', 'bedrooms']\n",
    "\n",
    "# The output/target of our model\n",
    "label = 'log_price'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bf565-af4e-49d4-bf45-ca1e1baf971f",
   "metadata": {},
   "source": [
    "# Read hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcd5911-9459-4372-a811-629a85e76565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make display fancy\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3888e74-ede8-4f55-a488-43102e05000a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>log_price</th><th>property_type</th><th>room_type</th><th>amenities</th><th>accommodates</th><th>bathrooms</th><th>bed_type</th><th>cancellation_policy</th><th>cleaning_fee</th><th>city</th><th>description</th><th>first_review</th><th>host_has_profile_pic</th><th>host_identity_verified</th><th>host_response_rate</th><th>host_since</th><th>instant_bookable</th><th>last_review</th><th>latitude</th><th>longitude</th><th>name</th><th>neighbourhood</th><th>number_of_reviews</th><th>review_scores_rating</th><th>thumbnail_url</th><th>zipcode</th><th>beds</th><th>bedrooms</th></tr>\n",
       "<tr><td>17761581</td><td>5.159055299214528</td><td>Apartment</td><td>Entire home/apt</td><td>{&quot;Cable TV&quot;,&quot;Wire...</td><td>6</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>false</td><td>Boston</td><td>Located in the he...</td><td>2016-01-01</td><td>true</td><td>false</td><td>54.0</td><td>2015-12-18</td><td>false</td><td>2017-07-25</td><td>42.35481432026883</td><td>-71.0746523547689</td><td>New Beacon Back B...</td><td>Back Bay</td><td>35</td><td>91.0</td><td>https://a0.muscac...</td><td>02116</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>11106074</td><td>5.1647859739235145</td><td>Apartment</td><td>Entire home/apt</td><td>{Internet,&quot;Wirele...</td><td>5</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>NYC</td><td>My place is close...</td><td>2017-04-30</td><td>true</td><td>true</td><td>90.0</td><td>2012-09-22</td><td>false</td><td>2017-09-11</td><td>40.68727263224503</td><td>-73.985738203789</td><td>Stylish centrally...</td><td>Boerum Hill</td><td>16</td><td>93.0</td><td>https://a0.muscac...</td><td>11201</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>17173284</td><td>5.1929568508902095</td><td>Apartment</td><td>Entire home/apt</td><td>{Internet,&quot;Wirele...</td><td>4</td><td>1.0</td><td>Real Bed</td><td>moderate</td><td>false</td><td>LA</td><td>two bedrooms with...</td><td>2013-03-15</td><td>true</td><td>false</td><td>100.0</td><td>2013-01-29</td><td>false</td><td>2017-04-15</td><td>34.09230262454207</td><td>-118.2722475650489</td><td>Amazing View in S...</td><td>Silver Lake</td><td>84</td><td>97.0</td><td>https://a0.muscac...</td><td>90026</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>7294049</td><td>5.703782474656202</td><td>Townhouse</td><td>Entire home/apt</td><td>{&quot;Wireless Intern...</td><td>7</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>NYC</td><td>Clean and simple ...</td><td>null</td><td>true</td><td>true</td><td>100.0</td><td>2012-12-13</td><td>false</td><td>null</td><td>40.740201083855354</td><td>-73.99899971053048</td><td>Clean + Simple Ch...</td><td>Chelsea</td><td>0</td><td>null</td><td>https://a0.muscac...</td><td>10011.0</td><td>6.0</td><td>2</td></tr>\n",
       "<tr><td>16738143</td><td>5.5174528964647065</td><td>Apartment</td><td>Entire home/apt</td><td>{&quot;Wireless Intern...</td><td>5</td><td>1.0</td><td>Real Bed</td><td>moderate</td><td>true</td><td>SF</td><td>Large living/play...</td><td>2017-07-31</td><td>true</td><td>true</td><td>100.0</td><td>2013-08-15</td><td>false</td><td>2017-08-11</td><td>37.77213223870029</td><td>-122.43379710422009</td><td>Family and kid fr...</td><td>Lower Haight</td><td>4</td><td>90.0</td><td>https://a0.muscac...</td><td>94117.0</td><td>3.0</td><td>2</td></tr>\n",
       "<tr><td>15421244</td><td>5.796057750765373</td><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>4</td><td>2.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>LA</td><td>Built in 1925, th...</td><td>null</td><td>true</td><td>true</td><td>100.0</td><td>2011-06-11</td><td>false</td><td>null</td><td>34.09847643495662</td><td>-118.25521367750434</td><td>Silver Lake Hillt...</td><td>Silver Lake</td><td>0</td><td>null</td><td>https://a0.muscac...</td><td>90039</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>15266605</td><td>5.5606816310155285</td><td>Apartment</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>3</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>NYC</td><td>Ideal for a coupl...</td><td>2015-07-24</td><td>true</td><td>true</td><td>60.0</td><td>2013-09-18</td><td>false</td><td>2017-07-31</td><td>40.78985910942261</td><td>-73.9739339272807</td><td>Bright/Large 1BR ...</td><td>Upper West Side</td><td>12</td><td>89.0</td><td>null</td><td>10024</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>19608383</td><td>5.416100402204419</td><td>House</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>6</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>LA</td><td>You’ll love my pl...</td><td>2016-07-25</td><td>true</td><td>true</td><td>100.0</td><td>2015-01-04</td><td>false</td><td>2016-12-31</td><td>33.84448064632771</td><td>-118.38276225549444</td><td>Charming Home, Pr...</td><td>Redondo Beach</td><td>4</td><td>95.0</td><td>https://a0.muscac...</td><td>90277</td><td>6.0</td><td>2</td></tr>\n",
       "<tr><td>15805757</td><td>5.247024072160486</td><td>Apartment</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>5</td><td>2.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>LA</td><td>Spacious West Hol...</td><td>2015-08-01</td><td>true</td><td>true</td><td>100.0</td><td>2015-07-22</td><td>true</td><td>2017-04-16</td><td>34.08837249233463</td><td>-118.37471799030315</td><td>Cali Character in...</td><td>West Hollywood</td><td>47</td><td>90.0</td><td>https://a0.muscac...</td><td>90069</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>6603860</td><td>5.700443573390688</td><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>2</td><td>1.5</td><td>Real Bed</td><td>moderate</td><td>true</td><td>SF</td><td>Come visit San Fr...</td><td>null</td><td>true</td><td>true</td><td>null</td><td>2012-07-13</td><td>false</td><td>null</td><td>37.79223214047678</td><td>-122.41878698332764</td><td>Spacious 2BR apar...</td><td>Nob Hill</td><td>0</td><td>null</td><td>https://a0.muscac...</td><td>94109</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>18123700</td><td>4.787491742782046</td><td>Apartment</td><td>Entire home/apt</td><td>{&quot;Air conditionin...</td><td>3</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>NYC</td><td>Cozy and clean tw...</td><td>null</td><td>true</td><td>true</td><td>61.0</td><td>2015-04-21</td><td>false</td><td>null</td><td>40.759451205035404</td><td>-73.82297979447127</td><td>Cozy and Clean</td><td>Flushing</td><td>0</td><td>null</td><td>https://a0.muscac...</td><td>11355</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>18489661</td><td>5.0106352940962555</td><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Wireless Int...</td><td>4</td><td>2.0</td><td>Real Bed</td><td>moderate</td><td>true</td><td>LA</td><td>This apartment is...</td><td>2017-04-14</td><td>true</td><td>true</td><td>50.0</td><td>2014-08-03</td><td>false</td><td>2017-04-14</td><td>33.978293981284594</td><td>-118.41573182327564</td><td>Luxurious apartme...</td><td>Westchester/Playa...</td><td>1</td><td>80.0</td><td>https://a0.muscac...</td><td>90094</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>5061178</td><td>4.74493212836325</td><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>3</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>NYC</td><td>You are invited t...</td><td>2016-01-26</td><td>true</td><td>true</td><td>100.0</td><td>2011-02-15</td><td>true</td><td>2017-09-19</td><td>40.69227034777894</td><td>-73.9518749897855</td><td>Easy Access To Th...</td><td>Bedford-Stuyvesant</td><td>48</td><td>93.0</td><td>https://a0.muscac...</td><td>11206</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>14235245</td><td>4.6913478822291435</td><td>House</td><td>Private room</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>5</td><td>1.0</td><td>Real Bed</td><td>moderate</td><td>true</td><td>LA</td><td>With two guest be...</td><td>2014-06-07</td><td>true</td><td>true</td><td>100.0</td><td>2014-04-15</td><td>false</td><td>2017-04-25</td><td>34.23333953716877</td><td>-118.26972028432259</td><td>Families! 2 BR Sl...</td><td>Sunland/Tujunga</td><td>80</td><td>97.0</td><td>https://a0.muscac...</td><td>91214</td><td>3.0</td><td>2</td></tr>\n",
       "<tr><td>16216621</td><td>5.855071922202427</td><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Wireless Int...</td><td>4</td><td>2.0</td><td>Real Bed</td><td>flexible</td><td>false</td><td>SF</td><td>Fully furnished s...</td><td>2017-08-06</td><td>true</td><td>true</td><td>100.0</td><td>2014-03-25</td><td>true</td><td>2017-08-06</td><td>37.76955423299128</td><td>-122.45194401802428</td><td>2-Bed/2-Bath Vict...</td><td>Haight-Ashbury</td><td>1</td><td>null</td><td>https://a0.muscac...</td><td>94117</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>14430192</td><td>6.579251212010101</td><td>House</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>4</td><td>2.5</td><td>Real Bed</td><td>strict</td><td>true</td><td>LA</td><td>Stunning architec...</td><td>2014-09-25</td><td>true</td><td>true</td><td>100.0</td><td>2014-08-01</td><td>false</td><td>2017-04-17</td><td>34.06067499665741</td><td>-118.74463143492173</td><td>Glamour, Luxury, ...</td><td>null</td><td>63</td><td>99.0</td><td>https://a0.muscac...</td><td>90265</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>16190668</td><td>4.23410650459726</td><td>Apartment</td><td>Private room</td><td>{TV,&quot;Wireless Int...</td><td>10</td><td>1.0</td><td>Real Bed</td><td>flexible</td><td>true</td><td>LA</td><td>This newly renova...</td><td>null</td><td>true</td><td>true</td><td>100.0</td><td>2014-08-22</td><td>false</td><td>null</td><td>34.09519622952786</td><td>-118.32009405958469</td><td>Fun Retro Chic Ap...</td><td>Hollywood</td><td>0</td><td>null</td><td>https://a0.muscac...</td><td>90028</td><td>3.0</td><td>2</td></tr>\n",
       "<tr><td>5796374</td><td>5.991464547107983</td><td>Apartment</td><td>Entire home/apt</td><td>{&quot;Wireless Intern...</td><td>4</td><td>3.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>LA</td><td>Experience the si...</td><td>2013-06-17</td><td>true</td><td>true</td><td>100.0</td><td>2012-11-06</td><td>false</td><td>2017-04-06</td><td>33.99365827276853</td><td>-118.48028332883881</td><td>2 Bed/2Bath on th...</td><td>Venice</td><td>42</td><td>97.0</td><td>null</td><td>90291</td><td>2.0</td><td>2</td></tr>\n",
       "<tr><td>10635609</td><td>5.1647859739235145</td><td>House</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>5</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>true</td><td>LA</td><td>Charming two bedr...</td><td>2013-10-12</td><td>true</td><td>true</td><td>100.0</td><td>2013-08-12</td><td>false</td><td>2017-01-03</td><td>34.098694017391736</td><td>-118.21636120312007</td><td>Breakfast under T...</td><td>Mount Washington</td><td>16</td><td>96.0</td><td>https://a0.muscac...</td><td>90065</td><td>3.0</td><td>2</td></tr>\n",
       "<tr><td>1365597</td><td>4.174387269895637</td><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>4</td><td>1.0</td><td>Real Bed</td><td>moderate</td><td>true</td><td>NYC</td><td>A great apartment...</td><td>2015-05-13</td><td>true</td><td>true</td><td>100.0</td><td>2014-07-12</td><td>false</td><td>2017-09-14</td><td>40.55169536950578</td><td>-74.14635527589677</td><td>Cozy 1BD side apa...</td><td>Great Kills</td><td>41</td><td>95.0</td><td>https://a0.muscac...</td><td>10308</td><td>2.0</td><td>2</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+------------------+-------------+---------------+--------------------+------------+---------+--------+-------------------+------------+-------+--------------------+------------+--------------------+----------------------+------------------+----------+----------------+-----------+------------------+-------------------+--------------------+--------------------+-----------------+--------------------+--------------------+-------+----+--------+\n",
       "|      id|         log_price|property_type|      room_type|           amenities|accommodates|bathrooms|bed_type|cancellation_policy|cleaning_fee|   city|         description|first_review|host_has_profile_pic|host_identity_verified|host_response_rate|host_since|instant_bookable|last_review|          latitude|          longitude|                name|       neighbourhood|number_of_reviews|review_scores_rating|       thumbnail_url|zipcode|beds|bedrooms|\n",
       "+--------+------------------+-------------+---------------+--------------------+------------+---------+--------+-------------------+------------+-------+--------------------+------------+--------------------+----------------------+------------------+----------+----------------+-----------+------------------+-------------------+--------------------+--------------------+-----------------+--------------------+--------------------+-------+----+--------+\n",
       "| 9668948| 6.745236349484363|        House|Entire home/apt|{TV,\"Wireless Int...|          16|      4.0|Real Bed|             strict|        true|    NYC|My place is close...|  2016-10-09|                true|                 false|              78.0|2016-05-21|           false| 2017-05-20|40.767382436512406| -73.91785207613377|Huge 9BR House, 1...|             Astoria|                4|                90.0|                null|  11103|10.0|       9|\n",
       "|17016258|6.4457198193855785|    Apartment|Entire home/apt|{TV,\"Wireless Int...|          16|      6.0|Real Bed|             strict|        true|     LA|My place is close...|        null|                true|                  true|              88.0|2015-11-24|           false|       null|  34.1668650184941|-118.44923627908727|Combo apt up 30pe...|        Sherman Oaks|                0|                null|                null|  91411|16.0|       9|\n",
       "| 6930780| 6.683360945766275|        House|Entire home/apt|{TV,\"Cable TV\",In...|          16|      3.0|Real Bed|             strict|        true|Chicago|This home is loca...|  2016-06-05|                true|                  true|             100.0|2016-01-31|           false| 2017-04-30| 41.93959632377493| -87.72552971273485|Historic Irving P...|            Avondale|               15|               100.0|https://a0.muscac...|  60641|16.0|       9|\n",
       "| 8532876| 5.703782474656202|        House|Entire home/apt|{TV,\"Cable TV\",\"W...|          16|      3.0|Real Bed|             strict|        true|    NYC|We can accommodat...|  2012-08-24|                true|                  true|             100.0|2011-12-11|           false| 2017-02-13| 40.68174321549544| -73.91920666930687|HOT SPOT FOR 20 A...|  Bedford-Stuyvesant|               28|                83.0|https://a0.muscac...|  11233|16.0|       9|\n",
       "| 9496176| 5.521460917862246|         Loft|Entire home/apt|{Internet,\"Wirele...|          16|      3.0|Real Bed|           flexible|       false|     LA|ARQADE - An Immer...|        null|                true|                 false|              93.0|2016-07-09|           false|       null| 34.04229965556855| -118.2517050185704|ARQADE - An Immer...|                null|                0|                null|                null|  90014|16.0|       9|\n",
       "|18269285| 6.282266746896006|        House|Entire home/apt|{TV,Internet,\"Wir...|          16|      4.0|Real Bed|             strict|        true|     LA|European hostel s...|  2015-09-26|                true|                 false|             100.0|2014-09-16|           false| 2015-10-26| 34.05606023656541|  -118.304158650412|Huge home 15+ wit...|                null|                9|                98.0|https://a0.muscac...|  90005|15.0|       9|\n",
       "|12386579|7.3098814858247865|        House|Entire home/apt|                  {}|          16|      5.0|Real Bed|           flexible|       false|     LA|The Beastie Boys ...|        null|                true|                  true|              76.0|2015-08-27|           false|       null| 34.25085991916618|-118.35299753368312|Moroccan Mansion ...|                null|                0|                null|                null|   null| 9.0|       9|\n",
       "| 4218034| 7.377758908227872|        House|Entire home/apt|{Internet,\"Wirele...|          12|      4.0|Real Bed|             strict|       false|     SF|If you are lookin...|        null|                true|                  true|              null|2012-03-26|           false|       null| 37.77289249226632|-122.42618201475078|Huge Victorian- G...|        Hayes Valley|                0|                null|https://a0.muscac...|94102.0| 9.0|       9|\n",
       "|17761581| 5.159055299214528|    Apartment|Entire home/apt|{\"Cable TV\",\"Wire...|           6|      1.0|Real Bed|             strict|       false| Boston|Located in the he...|  2016-01-01|                true|                 false|              54.0|2015-12-18|           false| 2017-07-25| 42.35481432026883|  -71.0746523547689|New Beacon Back B...|            Back Bay|               35|                91.0|https://a0.muscac...|  02116| 2.0|       2|\n",
       "|11106074|5.1647859739235145|    Apartment|Entire home/apt|{Internet,\"Wirele...|           5|      1.0|Real Bed|             strict|        true|    NYC|My place is close...|  2017-04-30|                true|                  true|              90.0|2012-09-22|           false| 2017-09-11| 40.68727263224503|   -73.985738203789|Stylish centrally...|         Boerum Hill|               16|                93.0|https://a0.muscac...|  11201| 2.0|       2|\n",
       "|17173284|5.1929568508902095|    Apartment|Entire home/apt|{Internet,\"Wirele...|           4|      1.0|Real Bed|           moderate|       false|     LA|two bedrooms with...|  2013-03-15|                true|                 false|             100.0|2013-01-29|           false| 2017-04-15| 34.09230262454207| -118.2722475650489|Amazing View in S...|         Silver Lake|               84|                97.0|https://a0.muscac...|  90026| 2.0|       2|\n",
       "| 7294049| 5.703782474656202|    Townhouse|Entire home/apt|{\"Wireless Intern...|           7|      1.0|Real Bed|             strict|        true|    NYC|Clean and simple ...|        null|                true|                  true|             100.0|2012-12-13|           false|       null|40.740201083855354| -73.99899971053048|Clean + Simple Ch...|             Chelsea|                0|                null|https://a0.muscac...|10011.0| 6.0|       2|\n",
       "|16738143|5.5174528964647065|    Apartment|Entire home/apt|{\"Wireless Intern...|           5|      1.0|Real Bed|           moderate|        true|     SF|Large living/play...|  2017-07-31|                true|                  true|             100.0|2013-08-15|           false| 2017-08-11| 37.77213223870029|-122.43379710422009|Family and kid fr...|        Lower Haight|                4|                90.0|https://a0.muscac...|94117.0| 3.0|       2|\n",
       "|15421244| 5.796057750765373|        House|Entire home/apt|{TV,\"Cable TV\",In...|           4|      2.0|Real Bed|             strict|        true|     LA|Built in 1925, th...|        null|                true|                  true|             100.0|2011-06-11|           false|       null| 34.09847643495662|-118.25521367750434|Silver Lake Hillt...|         Silver Lake|                0|                null|https://a0.muscac...|  90039| 2.0|       2|\n",
       "|15266605|5.5606816310155285|    Apartment|Entire home/apt|{TV,Internet,\"Wir...|           3|      1.0|Real Bed|             strict|        true|    NYC|Ideal for a coupl...|  2015-07-24|                true|                  true|              60.0|2013-09-18|           false| 2017-07-31| 40.78985910942261|  -73.9739339272807|Bright/Large 1BR ...|     Upper West Side|               12|                89.0|                null|  10024| 2.0|       2|\n",
       "|19608383| 5.416100402204419|        House|Entire home/apt|{TV,Internet,\"Wir...|           6|      1.0|Real Bed|             strict|        true|     LA|You’ll love my pl...|  2016-07-25|                true|                  true|             100.0|2015-01-04|           false| 2016-12-31| 33.84448064632771|-118.38276225549444|Charming Home, Pr...|       Redondo Beach|                4|                95.0|https://a0.muscac...|  90277| 6.0|       2|\n",
       "|15805757| 5.247024072160486|    Apartment|Entire home/apt|{TV,Internet,\"Wir...|           5|      2.0|Real Bed|             strict|        true|     LA|Spacious West Hol...|  2015-08-01|                true|                  true|             100.0|2015-07-22|            true| 2017-04-16| 34.08837249233463|-118.37471799030315|Cali Character in...|      West Hollywood|               47|                90.0|https://a0.muscac...|  90069| 2.0|       2|\n",
       "| 6603860| 5.700443573390688|    Apartment|Entire home/apt|{TV,\"Cable TV\",In...|           2|      1.5|Real Bed|           moderate|        true|     SF|Come visit San Fr...|        null|                true|                  true|              null|2012-07-13|           false|       null| 37.79223214047678|-122.41878698332764|Spacious 2BR apar...|            Nob Hill|                0|                null|https://a0.muscac...|  94109| 2.0|       2|\n",
       "|18123700| 4.787491742782046|    Apartment|Entire home/apt|{\"Air conditionin...|           3|      1.0|Real Bed|             strict|        true|    NYC|Cozy and clean tw...|        null|                true|                  true|              61.0|2015-04-21|           false|       null|40.759451205035404| -73.82297979447127|      Cozy and Clean|            Flushing|                0|                null|https://a0.muscac...|  11355| 2.0|       2|\n",
       "|18489661|5.0106352940962555|    Apartment|Entire home/apt|{TV,\"Wireless Int...|           4|      2.0|Real Bed|           moderate|        true|     LA|This apartment is...|  2017-04-14|                true|                  true|              50.0|2014-08-03|           false| 2017-04-14|33.978293981284594|-118.41573182327564|Luxurious apartme...|Westchester/Playa...|                1|                80.0|https://a0.muscac...|  90094| 2.0|       2|\n",
       "+--------+------------------+-------------+---------------+--------------------+------------+---------+--------+-------------------+------------+-------+--------------------+------------+--------------------+----------------------+------------------+----------+----------------+-----------+------------------+-------------------+--------------------+--------------------+-----------------+--------------------+--------------------+-------+----+--------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb = spark.read.format(\"avro\").table('team23_projectdb.airbnb_part_buck')\n",
    "airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f82970-a6b6-437e-92a2-b34ef1436e59",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o117.count.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)\n\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:968)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:963)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)\n\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)\n\t\t\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)\n\t\t\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)\n\t\t\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:968)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:963)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t... 7 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 70 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=__HIVE_DEFAULT_PARTITION__\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t... 10 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 50 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=__HIVE_DEFAULT_PARTITION__\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t... 10 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 50 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\t\t\t... 39 more\n\t\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t\t... 10 more\n\t\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t\t... 50 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 38 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\nCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 58 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9c490d24c4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairbnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mairbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9c490d24c4c3>\u001b[0m in \u001b[0;36mcnull\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# count null values across columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairbnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9c490d24c4c3>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# count null values across columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairbnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o117.count.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)\n\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:968)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:963)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)\n\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)\n\t\t\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)\n\t\t\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)\n\t\t\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:968)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:963)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t... 7 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 70 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=__HIVE_DEFAULT_PARTITION__\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t... 10 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 50 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=__HIVE_DEFAULT_PARTITION__\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t... 10 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 50 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\t\t\t... 39 more\n\t\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t\t... 10 more\n\t\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t\t... 50 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 38 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\nCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 58 more\n"
     ]
    }
   ],
   "source": [
    "# calculate uniques for a column (unused)\n",
    "def uniques(df, col):\n",
    "    return list(map(lambda x: x[col], airbnb.select(col).distinct().collect()))\n",
    "\n",
    "# count null values across columns\n",
    "def cnull(df):\n",
    "    return {col:df.filter(df[col].isNull()).count() for col in df.columns}\n",
    "\n",
    "display(cnull(airbnb))\n",
    "airbnb.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e05e6d-b953-4dd6-84ae-dbeac729dc76",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00984a8d-8516-47b6-891a-85711cb5f210",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o151.showString.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)\n\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:968)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:963)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t... 7 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 35 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:170)\n\t\t\t... 10 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:170)\n\t\t\t\t... 10 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 38 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\t\t\t... 39 more\n\t\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t\t... 10 more\n\t\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t\t... 50 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 38 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\nCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 58 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m             return self._jdf.showString(\n\u001b[1;32m    508\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 self.sql_ctx._conf.replEagerEvalTruncate(), vertical)\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"DataFrame[%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o151.showString.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)\n\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:968)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:963)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t... 7 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 35 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:170)\n\t\t\t... 10 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:170)\n\t\t\t\t... 10 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 38 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\t\t\t... 39 more\n\t\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t\t... 10 more\n\t\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t\t... 50 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 38 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\nCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 58 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o151.getRowsToPython.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)\n\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:968)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:963)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=__HIVE_DEFAULT_PARTITION__\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t... 7 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=__HIVE_DEFAULT_PARTITION__\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 35 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t... 10 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 50 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\t... 7 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 35 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\t\t\t... 39 more\n\t\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\t\t... 7 more\n\t\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t\t... 35 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 38 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\nCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 58 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mmax_num_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             sock_info = self._jdf.getRowsToPython(\n\u001b[0;32m--> 523\u001b[0;31m                 max_num_rows, self.sql_ctx._conf.replEagerEvalTruncate())\n\u001b[0m\u001b[1;32m    524\u001b[0m             \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o151.getRowsToPython.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:389)\n\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult(Tasks.scala:423)\n\tat scala.collection.parallel.ForkJoinTasks.executeAndWaitResult$(Tasks.scala:416)\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:60)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:968)\n\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:963)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=__HIVE_DEFAULT_PARTITION__\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t... 7 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=__HIVE_DEFAULT_PARTITION__\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 35 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=6.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)\n\t\t\t\tat java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.helpStealer(ForkJoinPool.java:1958)\n\t\t\t\tat java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:2047)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:390)\n\t\t\t\tat java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.ForkJoinTasks$WrappedTask.sync$(Tasks.scala:379)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:440)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:174)\n\t\t\t\t... 10 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=2.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 50 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=3.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=10.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\n\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)\n\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)\n\t\t... 7 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\t... 7 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=9.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 35 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=4.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\t\t... 39 more\n\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\t\t\t... 39 more\n\t\t\t\tSuppressed: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\t\t\t\t\tat org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(AvroContainerInputFormat.java:42)\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\t\t\t\t\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\t\t\t\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\t\t\t\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1(UnionRDD.scala:85)\n\t\t\t\t\tat org.apache.spark.rdd.UnionRDD.$anonfun$getPartitions$1$adapted(UnionRDD.scala:85)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner(RemainsIterator.scala:116)\n\t\t\t\t\tat scala.collection.parallel.AugmentedIterableIterator.map2combiner$(RemainsIterator.scala:113)\n\t\t\t\t\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.map2combiner(ParVector.scala:66)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.leaf(ParIterableLike.scala:1064)\n\t\t\t\t\tat scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)\n\t\t\t\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\t\t\t\tat scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)\n\t\t\t\t\tat scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)\n\t\t\t\t\tat scala.collection.parallel.ParIterableLike$Map.tryLeaf(ParIterableLike.scala:1061)\n\t\t\t\t\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:153)\n\t\t\t\t\t... 7 more\n\t\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=0.0\n\t\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t\t... 35 more\n\t\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=7.0\n\t\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t\t... 38 more\n\t\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=1.0\n\t\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t\t... 38 more\n\tCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=8.0\n\t\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t\t... 38 more\nCaused by: java.io.IOException: Input path does not exist: hdfs://hadoop-02.uni.innopolis.ru:8020/user/team23/project/warehouse/airbnb_houses_part_buck/bedrooms=5.0\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 58 more\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# drop rows with missing values\n",
    "df2 = airbnb.select(features + [label]).na.drop()\n",
    "df2 = df2.withColumnRenamed('log_price', 'label')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67fb5298-0111-4b71-8763-c364730d85e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>property_type</th><th>room_type</th><th>amenities</th><th>accommodates</th><th>bathrooms</th><th>bed_type</th><th>cancellation_policy</th><th>cleaning_fee</th><th>city</th><th>description</th><th>host_has_profile_pic</th><th>host_identity_verified</th><th>host_since</th><th>instant_bookable</th><th>latitude</th><th>longitude</th><th>name</th><th>neighbourhood</th><th>number_of_reviews</th><th>zipcode</th><th>beds</th><th>bedrooms</th><th>label</th></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Wireless Int...</td><td>7.0</td><td>2.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>LA</td><td>这间四房的PUD位于和亚凯迪亚的交...</td><td>1.0</td><td>1.0</td><td>2017-02-01</td><td>1.0</td><td>34.130149873646424</td><td>-118.01142964459407</td><td>洛杉矶之家 Home by Arc...</td><td>Arcadia</td><td>0.0</td><td>91016</td><td>4.0</td><td>4.0</td><td>5.594711379601837</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,&quot;W...</td><td>6.0</td><td>3.5</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>DC</td><td>We live in a newl...</td><td>1.0</td><td>1.0</td><td>2015-10-23</td><td>0.0</td><td>38.971519809471495</td><td>-77.0109846035962</td><td>New Built Home in...</td><td>Takoma</td><td>5.0</td><td>20012</td><td>4.0</td><td>4.0</td><td>5.1929568508902095</td></tr>\n",
       "<tr><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>12.0</td><td>1.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>Chicago</td><td>I renovated this ...</td><td>1.0</td><td>1.0</td><td>2016-01-31</td><td>0.0</td><td>41.886205398796</td><td>-87.70363964577317</td><td>Urban Oasis @ Ful...</td><td>Garfield Park</td><td>50.0</td><td>60612</td><td>4.0</td><td>4.0</td><td>5.075173815233828</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>10.0</td><td>2.5</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>DC</td><td>SPECTACULAR VICTO...</td><td>1.0</td><td>1.0</td><td>2012-11-29</td><td>1.0</td><td>38.906976137949826</td><td>-77.02699276815332</td><td>SPECTACULAR HOME ...</td><td>Logan Circle</td><td>81.0</td><td>20005</td><td>5.0</td><td>4.0</td><td>6.214608098422191</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>8.0</td><td>4.5</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>LA</td><td>Located centrally...</td><td>1.0</td><td>1.0</td><td>2014-09-08</td><td>0.0</td><td>34.113917348336905</td><td>-118.3864051817073</td><td>Hollywood Hills C...</td><td>Laurel Canyon</td><td>6.0</td><td>90046</td><td>4.0</td><td>4.0</td><td>7.170119543449628</td></tr>\n",
       "<tr><td>Townhouse</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>8.0</td><td>3.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>NYC</td><td>My quaint &amp; histo...</td><td>1.0</td><td>0.0</td><td>2016-07-12</td><td>0.0</td><td>40.692738449610665</td><td>-73.96507684327614</td><td>Spacious &amp; Chic 4...</td><td>Clinton Hill</td><td>7.0</td><td>11205.0</td><td>5.0</td><td>4.0</td><td>6.003887067106539</td></tr>\n",
       "<tr><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Wireless Int...</td><td>8.0</td><td>2.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>NYC</td><td>Our luxury loft o...</td><td>1.0</td><td>1.0</td><td>2009-12-26</td><td>0.0</td><td>40.80164249642196</td><td>-73.93922139409575</td><td>SpaHa Loft: Enorm...</td><td>East Harlem</td><td>138.0</td><td>10035.0</td><td>6.0</td><td>4.0</td><td>5.416100402204419</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>8.0</td><td>2.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>LA</td><td>Charming Spanish ...</td><td>1.0</td><td>1.0</td><td>2008-10-16</td><td>0.0</td><td>34.132253038035465</td><td>-118.38372996051108</td><td>Laurel Canyon - T...</td><td>Studio City</td><td>0.0</td><td>91604</td><td>5.0</td><td>4.0</td><td>5.814130531825067</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Wireless Int...</td><td>7.0</td><td>1.5</td><td>Real Bed</td><td>moderate</td><td>1.0</td><td>LA</td><td>My place is close...</td><td>1.0</td><td>1.0</td><td>2016-04-21</td><td>0.0</td><td>34.111469945695035</td><td>-118.11555480580259</td><td>Cute House 10 min...</td><td>San Gabriel</td><td>4.0</td><td>91775</td><td>5.0</td><td>4.0</td><td>5.272999558563747</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>9.0</td><td>2.5</td><td>Real Bed</td><td>strict</td><td>0.0</td><td>LA</td><td>A Vacation Home F...</td><td>1.0</td><td>1.0</td><td>2012-07-17</td><td>0.0</td><td>34.18856610064488</td><td>-118.12231125876929</td><td>Beautiful &amp; Tranq...</td><td>Altadena</td><td>0.0</td><td>91001</td><td>7.0</td><td>4.0</td><td>6.551080335043403</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>8.0</td><td>3.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>DC</td><td>Amazing newly ren...</td><td>1.0</td><td>1.0</td><td>2014-03-03</td><td>0.0</td><td>38.95744539010831</td><td>-77.08215362513657</td><td>Gorgeous spaceous...</td><td>Friendship Heights</td><td>1.0</td><td>20016</td><td>6.0</td><td>4.0</td><td>5.991464547107983</td></tr>\n",
       "<tr><td>Villa</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>12.0</td><td>5.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>LA</td><td>My place is close...</td><td>1.0</td><td>0.0</td><td>2016-06-05</td><td>1.0</td><td>34.13201711771783</td><td>-118.3598330656884</td><td>Hollywood Hills M...</td><td>Cahuenga Pass</td><td>12.0</td><td>90068</td><td>6.0</td><td>4.0</td><td>6.38856140554563</td></tr>\n",
       "<tr><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Wireless Int...</td><td>6.0</td><td>1.0</td><td>Real Bed</td><td>flexible</td><td>1.0</td><td>NYC</td><td>Stedet mitt er næ...</td><td>1.0</td><td>0.0</td><td>2014-02-24</td><td>0.0</td><td>40.706907573522614</td><td>-74.00801265208976</td><td>Luxury apartment ...</td><td>Financial District</td><td>0.0</td><td>10038</td><td>4.0</td><td>4.0</td><td>6.214608098422191</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>8.0</td><td>3.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>LA</td><td>The Space Beautif...</td><td>1.0</td><td>1.0</td><td>2015-05-26</td><td>0.0</td><td>34.28382039167145</td><td>-118.55584385360771</td><td>Best LA Vacation ...</td><td>Porter Ranch</td><td>15.0</td><td>91326</td><td>4.0</td><td>4.0</td><td>5.631211781821365</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Wireless Int...</td><td>12.0</td><td>3.5</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>SF</td><td>Beautiful city oa...</td><td>1.0</td><td>0.0</td><td>2014-01-27</td><td>0.0</td><td>37.767922493903534</td><td>-122.43812319559136</td><td>Panoramic Views o...</td><td>Haight-Ashbury</td><td>2.0</td><td>94117</td><td>8.0</td><td>4.0</td><td>6.684611727667928</td></tr>\n",
       "<tr><td>Apartment</td><td>Entire home/apt</td><td>{TV,Internet,&quot;Wir...</td><td>7.0</td><td>2.0</td><td>Real Bed</td><td>moderate</td><td>1.0</td><td>SF</td><td>Amazing two story...</td><td>1.0</td><td>1.0</td><td>2011-07-25</td><td>0.0</td><td>37.802696856910856</td><td>-122.41061140549536</td><td>2 Story Bay Views...</td><td>North Beach</td><td>11.0</td><td>94133</td><td>5.0</td><td>4.0</td><td>6.620073206530356</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>14.0</td><td>3.5</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>DC</td><td>The 8th Street Su...</td><td>1.0</td><td>1.0</td><td>2010-03-29</td><td>0.0</td><td>38.94684302074509</td><td>-77.02294316544807</td><td>Modern Charm Wash...</td><td>Petworth</td><td>57.0</td><td>20011</td><td>7.0</td><td>4.0</td><td>5.733341276897748</td></tr>\n",
       "<tr><td>Apartment</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>13.0</td><td>2.5</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>NYC</td><td>Very Spacious &amp; A...</td><td>1.0</td><td>1.0</td><td>2013-12-28</td><td>0.0</td><td>40.66405453711681</td><td>-73.85675960647723</td><td>★Newly Renovated ...</td><td>East New York</td><td>64.0</td><td>11208.0</td><td>9.0</td><td>4.0</td><td>5.0106352940962555</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Cable TV&quot;,In...</td><td>8.0</td><td>2.5</td><td>Real Bed</td><td>moderate</td><td>1.0</td><td>SF</td><td>Large Edwardian h...</td><td>1.0</td><td>1.0</td><td>2013-03-05</td><td>0.0</td><td>37.75930657366118</td><td>-122.40162215268744</td><td>Potrero Hill Edwa...</td><td>Potrero Hill</td><td>0.0</td><td>94107</td><td>6.0</td><td>4.0</td><td>6.1092475827643655</td></tr>\n",
       "<tr><td>House</td><td>Entire home/apt</td><td>{TV,&quot;Wireless Int...</td><td>8.0</td><td>2.0</td><td>Real Bed</td><td>strict</td><td>1.0</td><td>DC</td><td>My place is close...</td><td>1.0</td><td>0.0</td><td>2016-08-30</td><td>0.0</td><td>38.91803352273255</td><td>-77.01447395610319</td><td>Home in the Heart...</td><td>Bloomingdale</td><td>10.0</td><td>20001</td><td>6.0</td><td>4.0</td><td>5.857933154483459</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------+---------------+--------------------+------------+---------+--------+-------------------+------------+-------+--------------------+--------------------+----------------------+----------+----------------+------------------+-------------------+--------------------+-----------------+-----------------+-------+----+--------+------------------+\n",
       "|property_type|      room_type|           amenities|accommodates|bathrooms|bed_type|cancellation_policy|cleaning_fee|   city|         description|host_has_profile_pic|host_identity_verified|host_since|instant_bookable|          latitude|          longitude|                name|    neighbourhood|number_of_reviews|zipcode|beds|bedrooms|             label|\n",
       "+-------------+---------------+--------------------+------------+---------+--------+-------------------+------------+-------+--------------------+--------------------+----------------------+----------+----------------+------------------+-------------------+--------------------+-----------------+-----------------+-------+----+--------+------------------+\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|        10.0|      2.0|Real Bed|             strict|         1.0|     LA|This beautiful ho...|                 1.0|                   0.0|2015-09-02|             0.0| 33.98828625236665|-118.47161842966526|5 Bed Beach Retre...|           Venice|             17.0|  90291| 5.0|     5.0|6.3543700407973525|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|         9.0|      2.0|Real Bed|             strict|         1.0| Boston|Our elegant three...|                 1.0|                   1.0|2010-01-19|             1.0| 42.31418505456127| -71.06000714910215|The Grand View | ...|       Dorchester|             36.0|  02125| 4.0|     5.0|5.1647859739235145|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|        15.0|      4.0|Real Bed|             strict|         0.0|     LA|My place is close...|                 1.0|                   0.0|2016-10-23|             1.0| 33.94407627246796| -118.1024784565332|The Green Two Sto...|           Downey|              0.0|  90241|10.0|     5.0| 6.336825731146441|\n",
       "|        House|Entire home/apt|{TV,Internet,\"Wir...|        14.0|      2.0|Real Bed|             strict|         1.0|    NYC|Huge 5 BEDROOM Ap...|                 1.0|                   1.0|2014-01-09|             0.0| 40.66860081867279| -73.93036671609553|5 Bedroom, 2 Bath...|    Crown Heights|            118.0|11213.0|12.0|     5.0|5.5174528964647065|\n",
       "|    Apartment|Entire home/apt|{TV,\"Cable TV\",In...|        16.0|      3.0|Real Bed|             strict|         1.0| Boston|Hey there!!! Look...|                 1.0|                   1.0|2014-08-13|             1.0| 42.32354905148078| -71.09446167390402|HUGE 5BR/3B apart...|          Roxbury|             78.0|  02119|10.0|     5.0| 6.551080335043403|\n",
       "|         Loft|Entire home/apt|{TV,\"Cable TV\",In...|        16.0|      4.0|Real Bed|             strict|         1.0|Chicago|This has been the...|                 1.0|                   1.0|2013-03-10|             1.0| 41.90499593267757| -87.63370146801714|5 Bedroom Downtow...|         Old Town|              5.0|  60610| 6.0|     5.0| 6.906754778648554|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|        14.0|      5.0|Real Bed|             strict|         1.0|     LA|Five-bedroom esta...|                 1.0|                   1.0|2014-07-20|             0.0| 34.07846565710667|-118.39980375014454|Beverly Hills Fla...|    Beverly Hills|              6.0|  90210| 6.0|     5.0| 7.003065458786463|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|        10.0|      5.5|Real Bed|             strict|         1.0|     LA|Elisa Estate gree...|                 1.0|                   1.0|2015-08-05|             1.0| 34.15063745112016|-118.48747988874187|Luxurious and Gat...|           Encino|             12.0|  91436| 5.0|     5.0|7.3132203870903005|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|         8.0|      3.0|Real Bed|             strict|         1.0|     LA|Lovely & Tranquil...|                 1.0|                   0.0|2015-04-21|             0.0| 34.05299103992546| -118.5213366355242|Pacific Palisades...|Pacific Palisades|              0.0|  90272| 4.0|     5.0| 6.620073206530356|\n",
       "|    Apartment|Entire home/apt|{TV,\"Cable TV\",In...|        10.0|      5.0|Real Bed|             strict|         1.0|    NYC|If you are lookin...|                 1.0|                   1.0|2013-06-26|             0.0|  40.7195362078293| -73.98860468503892|PRIME Lower East ...|  Lower East Side|             11.0|  10002| 5.0|     5.0| 5.075173815233828|\n",
       "|        House|Entire home/apt|{TV,\"Wireless Int...|         8.0|      2.5|Real Bed|           moderate|         1.0|    NYC|Newly renovated b...|                 1.0|                   1.0|2012-10-27|             1.0|40.677870315653685| -73.97225230925281|Modern Brooklyn b...| Prospect Heights|              3.0|  11238| 5.0|     5.0|6.1092475827643655|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",\"W...|        10.0|      3.0|Real Bed|           flexible|         1.0|     LA|Newly remodeled w...|                 1.0|                   1.0|2013-02-05|             0.0| 34.10196499016505|-118.27397483464952|Amazing 5 bdrm in...|        Los Feliz|              5.0|  90027| 5.0|     5.0| 6.309918278226516|\n",
       "|        House|Entire home/apt|{TV,\"Wireless Int...|         9.0|      2.0|Real Bed|             strict|         1.0|     LA|My place is close...|                 1.0|                   1.0|2010-08-31|             0.0|34.104551366235235|-118.15543037679664|5 bd/2bt Close to...|   South Pasadena|              0.0|  91030| 5.0|     5.0|  6.38856140554563|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|         6.0|      4.5|Real Bed|           flexible|         0.0|     DC|Located in Barnab...|                 1.0|                   0.0|2016-12-09|             1.0| 38.97371736747552| -77.06412664230905|Beautiful DC Hous...|      Chevy Chase|              0.0|  20015| 5.0|     5.0|5.0106352940962555|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|        14.0|      3.0|Real Bed|             strict|         0.0|Chicago|Perfect location ...|                 1.0|                   1.0|2015-07-31|             0.0|41.976582600294854| -87.65765091229237|Beautiful Modern ...|        Edgewater|             16.0|  60640| 7.0|     5.0| 6.421622267806519|\n",
       "|        House|Entire home/apt|{TV,\"Wireless Int...|        10.0|      2.0|Real Bed|           flexible|         1.0|     LA|My place is close...|                 1.0|                   0.0|2013-03-17|             1.0| 34.07793525352941|-118.37491985173651|5Bd-2BT house sho...|     Mid-Wilshire|              0.0|  90048| 5.0|     5.0| 6.214608098422191|\n",
       "|        Villa|Entire home/apt|{\"Cable TV\",\"Wire...|        10.0|      4.0|Real Bed|             strict|         1.0|     LA|The rates range d...|                 1.0|                   1.0|2015-07-01|             0.0|34.126737268897344|-118.35124079097451|Luxury Villa in H...|    Cahuenga Pass|              2.0|  90068| 5.0|     5.0|7.2078598714324755|\n",
       "|    Apartment|Entire home/apt|{TV,\"Cable TV\",In...|        12.0|      2.0|Real Bed|             strict|         1.0|    NYC|Our 5 bedroom, 2 ...|                 1.0|                   1.0|2011-09-12|             0.0|40.743194329463094| -73.98842180183992|Beautiful 5th Ave...|Flatiron District|             70.0|  10001| 7.0|     5.0|6.1070228877422545|\n",
       "|        House|Entire home/apt|{TV,\"Cable TV\",In...|        13.0|      2.5|Real Bed|             strict|         0.0|    NYC|Why pay a fortune...|                 1.0|                   1.0|2014-10-31|             0.0| 40.80520505526643| -73.94518514876454|Huge 5BR Townhous...|           Harlem|             75.0|  10027| 9.0|     5.0| 6.204557762568692|\n",
       "|    Townhouse|Entire home/apt|{TV,\"Cable TV\",In...|         9.0|      2.0|Real Bed|             strict|         1.0|     DC|You’ll love my pl...|                 1.0|                   1.0|2014-02-11|             0.0|38.922868042708494| -77.07355210213899|Townhouse /Glover...|      Glover Park|              0.0|  20007| 6.0|     5.0| 6.522092798170153|\n",
       "+-------------+---------------+--------------------+------------+---------+--------+-------------------+------------+-------+--------------------+--------------------+----------------------+----------+----------------+------------------+-------------------+--------------------+-----------------+-----------------+-------+----+--------+------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "categoricalCols = ['property_type', 'room_type', 'bed_type', 'cancellation_policy', 'city', 'neighbourhood', 'zipcode']\n",
    "textCols = ['name', 'description']\n",
    "dateCols = ['host_since']\n",
    "booleanCols = ['cleaning_fee', 'host_has_profile_pic', 'host_identity_verified', 'instant_bookable']\n",
    "numericalCols = ['accommodates', 'bathrooms', 'number_of_reviews', 'beds', 'bedrooms']\n",
    "geoCols = [['latitude', 'longitude']]\n",
    "jsonCols = ['amenities']\n",
    "\n",
    "# cast all boolean and numerical columns to the same type\n",
    "numericalCols += booleanCols\n",
    "for c in numericalCols:\n",
    "    df2 = df2.withColumn(c, df2[c].cast('float'))\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71bafe-c88b-47d2-accc-a9ca27f3a95d",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e536045-6712-4d6b-9c01-81f1d854e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for date features in YYYY-MM-DD format\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import pyspark.sql.functions as F\n",
    "import math\n",
    "    \n",
    "class YMDTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):  \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol: str = \"input\", outputCol: str = \"output\"):\n",
    "        super(YMDTransformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "  \n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_col = self.getInputCol()\n",
    "        output_col = self.getOutputCol()\n",
    "        \n",
    "        DAY_PERIOD = 31\n",
    "        MONTH_PERIOD = 12\n",
    "        \n",
    "        # split the data and cast to float\n",
    "        input_col = F.split(df[input_col], '-').cast(\"array<float>\")\n",
    "        \n",
    "        # apply transform to day\n",
    "        d_sin = F.sin(2 * math.pi * F.element_at(input_col, 3) / DAY_PERIOD)\n",
    "        d_cos = F.cos(2 * math.pi * F.element_at(input_col, 3) / DAY_PERIOD)\n",
    "        \n",
    "        # apply transform to month\n",
    "        m_sin = F.sin(2 * math.pi * F.element_at(input_col, 2) / MONTH_PERIOD)\n",
    "        m_cos = F.cos(2 * math.pi * F.element_at(input_col, 2) / MONTH_PERIOD)\n",
    "        \n",
    "        # year remains as is\n",
    "        y = F.element_at(input_col, 1)\n",
    "        \n",
    "        # pack everything into a vector for VectorAssembler\n",
    "        atov = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "        res = F.array(d_sin, d_cos, m_sin, m_cos, y)\n",
    "        res = atov(res)\n",
    "        \n",
    "        return df.withColumn(output_col, res)\n",
    "\n",
    "a = YMDTransformer(inputCol='123', outputCol=\"{0}_transformed\".format('rar'))\n",
    "\n",
    "ymd_transformers = [ YMDTransformer(inputCol=c, outputCol=\"{0}_transformed\".format(c)) for c in dateCols ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d865ba-00fa-465f-b170-57d0f9389f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for date features in YYYY-MM-DD format\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCols, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import pyspark.sql.functions as F\n",
    "import math\n",
    "    \n",
    "class ECEFTransformer(Transformer, HasInputCols, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):  \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols: str = \"input\", outputCol: str = \"output\"):\n",
    "        super(ECEFTransformer, self).__init__()\n",
    "        self.inputCols = inputCols\n",
    "        self.outputCol = outputCol\n",
    "  \n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_cols = self.getInputCols()\n",
    "        output_col = self.getOutputCol()\n",
    "        \n",
    "        a = 6378137.0;               # WGS-84 semi-major axis\n",
    "        e2 = 6.6943799901377997e-3;  # WGS-84 first eccentricity squared\n",
    "        \n",
    "        lat = df[input_cols[0]]\n",
    "        lon = df[input_cols[1]]\n",
    "        \n",
    "        n = a / F.sqrt(1 - e2 * F.sin(lat) * F.sin(lat));\n",
    "        x = n * F.cos(lat) * F.cos(lon);    # ECEF x\n",
    "        y = n * F.cos(lat) * F.sin(lon);    # ECEF y\n",
    "        z = (n * (1 - e2 ))* F.sin(lat);    # ECEF z\n",
    "        \n",
    "        # pack everything into a vector for VectorAssembler\n",
    "        atov = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "        res = F.array(x, y, z)\n",
    "        res = atov(res)\n",
    "        \n",
    "        return df.withColumn(output_col, res)\n",
    "\n",
    "ecef_transformers = [ ECEFTransformer(inputCols=c, outputCol=\"{0}_transformed\".format(c)) for c in geoCols ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "640da931-3353-4d37-848f-8e3d04c761db",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'YMDTransformer' object has no attribute 'host_since_transformed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e07452b736a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorizers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mymd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mymd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mymd_transformers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                             \u001b[0;34m[\u001b[0m\u001b[0mecef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mecef\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mecef_transformers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                             numericalCols, outputCol= \"features\")\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e07452b736a1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoders\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectorizers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                             \u001b[0;34m[\u001b[0m\u001b[0mymd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mymd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mymd_transformers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mecef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mecef\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mecef_transformers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                             numericalCols, outputCol= \"features\")\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36mgetOutputCol\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mGets\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutputCol\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mits\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \"\"\"\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mgetOrDefault\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mRaises\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneither\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolveParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_resolveParam\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot resolve %r as a param.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mgetParam\u001b[0;34m(self, paramName)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mGets\u001b[0m \u001b[0ma\u001b[0m \u001b[0mparam\u001b[0m \u001b[0mby\u001b[0m \u001b[0mits\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'YMDTransformer' object has no attribute 'host_since_transformed'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Tokenize textual features by words\n",
    "tokenizers = [ RegexTokenizer(inputCol=c, outputCol=\"{0}_tokens\".format(c), pattern=\" \") for c in textCols ]\n",
    "tokenizers += [ RegexTokenizer(inputCol=c, outputCol=\"{0}_tokens\".format(c), pattern=\"[\\\",{}]+\") for c in jsonCols ]\n",
    "\n",
    "# Vectorize them\n",
    "vectorizers = [ Word2Vec(vectorSize=50, seed=42, minCount=1, inputCol=tokenizer.getOutputCol(), outputCol=\"{0}_vectorized\".format(tokenizer.getOutputCol())) for tokenizer in tokenizers ]\n",
    "\n",
    "# Create String indexer to assign index for the string fields where each unique string will get a unique index\n",
    "# String Indexer is required as an input for One-Hot Encoder \n",
    "# We set the case as `skip` for any string out of the input strings\n",
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in categoricalCols ]\n",
    "\n",
    "# Encode the strings using One Hot encoding\n",
    "# default setting: dropLast=True ==> For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].\n",
    "encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "\n",
    "# This will concatenate the input cols into a single column.\n",
    "assembler = VectorAssembler(inputCols= \\\n",
    "                            [encoder.getOutputCol() for encoder in encoders] +\\\n",
    "                            [vectorizer.getOutputCol() for vectorizer in vectorizers] +\\\n",
    "                            [ymd.getOutputCol() for ymd in ymd_transformers] +\\\n",
    "                            [ecef.getOutputCol() for ecef in ecef_transformers] +\\\n",
    "                            numericalCols, outputCol= \"features\")\n",
    "\n",
    "# Apply PCA to reduce dimetionalty and reduce computation time\n",
    "pca = PCA(k=200, inputCol='features', outputCol='components')\n",
    "\n",
    "# You can create a pipeline to use only a single fit and transform on the data.\n",
    "pipeline = Pipeline(stages=ymd_transformers + ecef_transformers + tokenizers + vectorizers + indexers + encoders + [assembler] + [pca])\n",
    "\n",
    "\n",
    "# Fit the pipeline ==> This will call the fit functions for all transformers if exist\n",
    "model = pipeline.fit(df2)\n",
    "# Fit the pipeline ==> This will call the transform functions for all transformers\n",
    "data = model.transform(df2)\n",
    "\n",
    "display(data)\n",
    "\n",
    "# We delete all features and keep only the features and label columns\n",
    "transformed = data.select([\"components\", \"label\"])\n",
    "transformed = transformed.withColumnRenamed('components', 'features')\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4\n",
    "# distinct values are treated as continuous.\n",
    "#featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=1000).fit(data)\n",
    "#transformed = featureIndexer.transform(data)\n",
    "\n",
    "# Display the output Spark DataFrame\n",
    "display(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd3df2-8f31-42ed-8f0f-7dfcb795a5e8",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19728e0-7c3c-416f-a098-b0f40ec77de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split the data into 60% training and 40% test (it is not stratified)\n",
    "(train_data, test_data) = transformed.randomSplit([0.6, 0.4], seed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83702a4-c8bc-40a4-9ef5-f026714370f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaced coalesce(1) with repartition(1) to fix OoM issue\n",
    "def run(command):\n",
    "    import os\n",
    "    return os.popen(command).read()\n",
    "\n",
    "train_data.select(\"features\", \"label\")\\\n",
    "    .repartition(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > data/train.json\")\n",
    "\n",
    "test_data.select(\"features\", \"label\")\\\n",
    "    .repartition(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/test/*.json > data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fecd52-189d-45af-841c-47819b311918",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daffa79-5f41-4e78-aa1a-c914f8b3f1f5",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892ef1c-f7c4-4acd-8d7d-5630f0823c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Create Linear Regression Model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_lr = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1396-77d4-4dd8-8777-d0e6fcbd629f",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0f1ff-a175-4e00-ac1b-a5476dd7dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_lr.transform(test_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e97403-59a0-4ec4-b044-678ba6aa2b66",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b166ca-8b62-4995-adca-129021dd6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator1_rmse.evaluate(predictions)\n",
    "r2 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse))\n",
    "print(\"R^2 on test data = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0e075-c37c-492a-b894-2f64d4dea72b",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04437d-2bf9-4803-83cc-b7ed17d311e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4388f9-db94-423c-879d-392c90041d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(\n",
    "                    model_lr.aggregationDepth, [2, 3, 4])\\\n",
    "                    .addGrid(model_lr.regParam, np.logspace(1e-3,1e-1)\n",
    "                    )\\\n",
    "                    .build()\n",
    "\n",
    "cv = CrossValidator(estimator = lr, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = evaluator1_rmse,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=2)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27e31c-959e-438c-bec0-1f109d7e2c15",
   "metadata": {},
   "source": [
    "## Best model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86fc18-eb29-4f03-bfef-ed3aa1daa4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model1 = bestModel\n",
    "pprint(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f666a-f792-4b50-b38f-b39f9208ab25",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562da25-53d1-46dc-ac5d-68a6e9041927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.write().overwrite().save(\"project/models/model1\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model1 models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9b220-54e1-49ce-802c-fc005d8f63c3",
   "metadata": {},
   "source": [
    "## Predict for test data using best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a05de-dcae-4bb0-8cf1-de760a62e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096936c5-a575-48b7-92d2-be22893e892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .repartition(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model1_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model1_predictions.csv/*.csv > output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9a43d-80ce-4ec1-930f-671a240673d9",
   "metadata": {},
   "source": [
    "## Evaluate the best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cc0e6-7d0d-4030-a372-375e5e9b84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse1 = evaluator1_rmse.evaluate(predictions)\n",
    "r21 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse1))\n",
    "print(\"R^2 on test data = {}\".format(r21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308587c-3e52-41c6-ba7d-494d53cc45b1",
   "metadata": {},
   "source": [
    "# Second model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929b5a0-a159-4c94-ae56-086d3cf52df0",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8981a-e274-4ab7-9d03-fefdc068b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Create Linear Regression Model\n",
    "gbt = GBTRegressor()\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_gbt = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c9a67-fe61-49fe-898f-017df0c4006b",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb95001-9d15-4da4-98df-2b87c0c39200",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_gbt.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94da6f-f913-45ec-a377-d9c42dacb075",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3508997-af5a-4882-8fd6-8d0a5be07ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R^2 on test data = {}\".format(r22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a2d06-39bd-4c71-b0cc-18a63d995949",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e50b9-5090-45ed-bfa7-7ddca0706176",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbt.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea6a22-9f7a-483b-818e-0b4e62f723e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(model_gbt.maxDepth, [2, 5]).addGrid(model_gbt.lossType, ['squared', 'absolute']).build()\n",
    "\n",
    "cv = CrossValidator(estimator = gbt, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = evaluator2_rmse,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=2)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524320f-8764-4ed4-b8ff-9d0a43b2ee29",
   "metadata": {},
   "source": [
    "## Best model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccafef7f-5a51-4370-8572-447f06b92248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model2 = bestModel\n",
    "pprint(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a3f34-cd0b-4e7e-a0ff-8670bee1e2ef",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bb094-67c3-4a8b-a52a-95a1f7c7bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model2 models/model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220eee3-99d3-44cc-82ab-b941fd5e5e7c",
   "metadata": {},
   "source": [
    "## Predict for test data using best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53789d15-33a1-43b2-8e8e-df7627df5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df5365-5d86-407f-b71a-878fff80b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .repartition(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model2_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model2_predictions.csv/*.csv > output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9e394-a65a-4cb3-b9c8-1036fd3a443a",
   "metadata": {},
   "source": [
    "## Evaluate the best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83b832-0088-4e95-95fc-768bc13d70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R^2 on test data = {}\".format(r22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd7b48-f536-495a-ac38-90ea1b9014af",
   "metadata": {},
   "source": [
    "# Compare best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91f3c3-50ca-4ce1-9dfa-dba7449fbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[str(model1),rmse1, r21], [str(model2),rmse2, r22]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008d30b-9489-44a2-84a0-95bcd9608319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > output/evaluation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
